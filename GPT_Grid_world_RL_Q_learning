import numpy as np

# Set up grid dimensions and starting position
grid_size = (5, 5)
start_pos = (0, 0)

# Define the actions available in the grid
actions = ["up", "down", "left", "right"]

# Define the rewards for each possible action and state
rewards = np.zeros(grid_size + (len(actions),))
rewards[4, 4, :] = 1.0  # Reward for reaching the end goal
rewards[:, 0, 0] = -1.0  # Penalty for going out of bounds
rewards[:, -1, 1] = -1.0  # Penalty for going out of bounds
rewards[0, :, 2] = -1.0  # Penalty for going out of bounds
rewards[-1, :, 3] = -1.0  # Penalty for going out of bounds

# Set up Q-table
q_table = np.zeros(grid_size + (len(actions),))

# Define the learning rate and discount factor
alpha = 0.1
gamma = 0.9

# Initialize current position and number of iterations
pos = start_pos
iterations = 1000

# Start Q-learning loop
for i in range(iterations):
    # Select an action at random
    action = np.random.randint(len(actions))
    
    # Update Q-value for current state and action
    q_table[pos[0], pos[1], action] = (1 - alpha) * q_table[pos[0], pos[1], action] + \
        alpha * (rewards[pos[0], pos[1], action] + gamma * np.max(q_table[pos[0], pos[1], :]))
    
    # Take the selected action and update the current position
    if actions[action] == "up":
        pos = (max(pos[0] - 1, 0), pos[1])
    elif actions[action] == "down":
        pos = (min(pos[0] + 1, grid_size[0] - 1), pos[1])
    elif actions[action] == "left":
        pos = (pos[0], max(pos[1] - 1, 0))
    elif actions[action] == "right":
        pos = (pos[0], min(pos[1] + 1, grid_size[1] - 1))

# Print final Q-table
print(q_table)